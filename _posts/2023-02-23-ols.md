---
layout: post
title:  Ordinary least squares
date: 2023-02-23 16:00:00
description: A Python-coded implementation of OLS for fitting a linear regression model to simulated data
tags: machine_learning ols regression modeling statistics
categories: tutorials code
---

Ordinary least squares (OLS) is a method for fitting a linear regression model using least squares - that is, minimizing the sum of the squared differences between the true vs. predicted outputs of the linear model to determine the weights that optimize the model fit.

# Simple linear regression model
A linear regression model for a sample $$i$$ can be formulated as:

$$
y_i = \alpha + \beta x_i + \varepsilon _i
$$ 

where $$(\alpha,\beta)$$ are the model parameters, $$x_i$$ is a scalar predictor variable, and $$y_i$$ is a response variable. $$\varepsilon_i$$ is a sample specific error term. 

# The goal of OLS
The goal of OLS is to estimate the values of $$(\alpha, \beta)$$ in the model given a set of training data so as to make predictions of $$\hat{y}_i=\alpha + \beta x_i$$. In order to estimate the parameter values, OLS mimizes the **sum of squared errors** between the observed vs. predicted response variables for each predictor $x_i$: 

$$
SSE = \sum _{i=1} ^N (y_i - \hat{y}_i)^2
$$ 

In the case of simple linear regression, we can estimate the parameters as follows:


$$\beta = \frac{\sum_{i=1}^{N} \left(x_i-\bar{\mathbf{x}} \right) \left(y_i-\bar{\mathbf{y}} \right)}{\sum _{i=1}^{N} \left( x_i - \bar{\mathbf{x}} \right)^2} = \frac{Cov(x,y)}{Var(x)}
$$

$$ 
\alpha = \bar{\mathbf{y}} - \beta \bar{\mathbf{x}} 
$$

# Simulate data
Let's start by simulating some data for this tutorial. We will randomly generate 100 values for each $$\mathbf{y}$$ and $$\mathbf{x}$$. We will also generate a residual for each value to simulate some noise ($$\varepsilon$$) in the linear relationship between $$\mathbf{x}$$ and $$\mathbf{y}$$. We will use predetermined values of $$\alpha = 2$$ and $$\beta = 0.3$$ for generating these values.

{% highlight python linenos %}
 import numpy as np
 import matplotlib.pyplot as plt
 
 # random seed
 np.random.seed(0)
 
 # define alpha and beta
 alpha = 2
 beta = 0.3
 
 # generate x, y
 x = 2 * np.random.randn(100) + 3   # generate 100 values of x with mean = 2, stddev = 3
 res = 0.5 * np.random.randn(100)   # Generate 100 error terms
 y = 2 + 0.3 * x + res   # calculate y
{% endhighlight %}

# Estimate $$\hat{\alpha}$$ and $$\hat{\beta}$$
{% highlight python linenos %}
 # calculate mean of x and y
 xbar = np.mean(x)
 ybar = np.mean(y)

 # estimate beta and alpha from data 
 betaHat = np.sum((x-xbar)*(y-ybar))/np.sum((x-xbar)**2)
 alphaHat = ybar - betaHat*xbar
 
 print('betaHat = %.5f, alphaHat = %.5f' % (betaHat, alphaHat))
{% endhighlight %}

Check and see how well these estimated values of $$\hat{\alpha}$$ and $$\hat{\beta}$$ match up against the values of $$\alpha=2$$ and $$\beta=0.3$$ that we used to generate this simulated dataset! 

# Predict $$\hat{\mathbf{y}}$$ 
Now that we have fitted our simple linear regression model, we can use the estimated parameters to make estimates of $$\hat{\mathbf{y}}$$ given the predictors $$\mathbf{x}$$.
{% highlight python linenos %}
 yhat = alphaHat + betaHat*x
{% endhighlight %}

Let's plot the estimates of $$\hat{\mathbf{y}}$$ against the true response variables $$\mathbf{y}$$:

{% highlight python linenos %}
 plt.scatter(x, y)   # scatter plot showing actual data
 plt.plot(x, yhat, 'r')     # regression line
 plt.title('Predicted vs. Actual values')
 plt.xlabel('X')
 plt.ylabel('y')

 plt.show()
{% endhighlight %}
It should look something like this:

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/ols/true_vs_pred_scatterplot.png" class="img-fluid" %}
    </div>
</div>

We can also check the Pearson correlation between $$\hat{\mathbf{y}}$$ and $$\mathbf{y}$$:

{% highlight python linenos %}
 from scipy.stats import pearsonr

 corr, _ = pearsonr(yhat, y)
 print("Pearson's r = %.3f" % corr)
{% endhighlight %}
