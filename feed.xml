<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://zrcjessica.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zrcjessica.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-28T20:32:56+00:00</updated><id>https://zrcjessica.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Generalized linear models</title><link href="https://zrcjessica.github.io/blog/2023/glm/" rel="alternate" type="text/html" title="Generalized linear models"/><published>2023-02-28T11:49:00+00:00</published><updated>2023-02-28T11:49:00+00:00</updated><id>https://zrcjessica.github.io/blog/2023/glm</id><content type="html" xml:base="https://zrcjessica.github.io/blog/2023/glm/"><![CDATA[<p>Generalized linear models (GLMs) are something I’ve worked a lot with during my PhD. This tutorial is adapted from a lesson I developed for the bootcamp course I taught for first years in my PhD program. A Jupyter notebook for this tutorial can be found on my <a href="https://github.com/zrcjessica/ml_concepts/blob/main/GLM_tutorial.ipynb">GitHub</a>. As the name implies, GLMs are a generalized form of linear regression. Linear regression models make a number of assumptions about the data:</p> <ul> <li>there must be a linear relationship between the response variable and the predictors (<strong>linearity</strong>)</li> <li>the predictor variables, \(x\), can be treated as fixed values rather than random variables (<strong>exogeneity</strong>)</li> <li>the variance of the errors does not depend on the values of the predictorr variables, \(x\) (<strong>homoscedacity</strong>)</li> <li>the errors of the response variables, \(y\), are not correlated with one another (<strong>independence of errors</strong>)</li> <li>there does not exist a linear relationship between two or more of the predictor variables (<strong>lack of perfect multicollinearity</strong>)</li> </ul> <h1 id="ordinary-linear-regression-and-its-assumptions">Ordinary linear regression and its assumptions</h1> <p>Ordinary linear regression predicts the expected value of a response variable, \(y\), as a linear combination of predictors, \(\mathbf{x}\). That is, for a given data point \(i\) in a data set of size \(n\), \(\{ y_i,x_{i1}, ..., x_{ip} \} _{i=1}^{n}\), a linear regression model assumes that:</p> \[y_i = \beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip} + \varepsilon _i = \mathbf{x}_i ^T \mathbf{\beta} \forall i=1,...,n\] <p>where \(\varepsilon\) is an error term that adds noise to the linear relationship between the predictors and the response variables. This expression can also be expressed in matrix notation as</p> \[\mathbf{y} = \mathbf{X \beta} + \mathbf{\varepsilon}\] <p><a href="https://en.wikipedia.org/wiki/Linear_regression">Source</a></p> <p>One of the implications of ordinary linear regression is that a constant change in the predictors leads to a constant change in the response variables (a <strong>linear-response model</strong>). However, this assumption is violated by certain types of repsonse variables. For example, when the response variable is restricted to being a positive value, or when the response variable is non-linear with relation to the predictors, or when the response is binary or categorical. Generalized linear models solve this problem by allowing for response variables that are not restricted to being normally distributed. Instead, it allows for some function of the response variable, known as the <strong>link function</strong>, to have a linear relationship with the predictors.</p> <h1 id="glms-and-link-functions">GLMs and link functions</h1> <p>In a GLM, it is assumed that each response variable \(y\) comes from some statistical distribution in the <a href="https://en.wikipedia.org/wiki/Exponential_family">exponential family</a>. These inclued, but are not limited to, the normal distribution. The mean, or expected value, of the response variable \(y\) given the predictors \(\mathbf{X}\) can be expressed as:</p> \[E(\mathbf{y}|\mathbf{X}) = \mathbf{\mu} = g^{-1}(\mathbf{X}\beta)\] <p>Here, \(g\) is a link function that relates the <strong>linear predictor</strong> \(\mathbf{X \beta}\) to the expected value of the response variable \(y\) conditional upon the predictors \(\mathbf{X}\). The link function used depends on the distribution of the response variables you’re working with.</p> <h1 id="load-data-for-tutorial">Load data for tutorial</h1> <p>In this tutorial, we will work with a dataset containing information about California standardized testing results (STAR test) for grades 2-11. This dataset is loaded with the <a href="https://www.statsmodels.org/dev/datasets/generated/star98.html">statsmodels package</a> and contains test results for 303 unified school districts. Here, the binary response variables respresent the number of 9th graders scoring above the national median for the math section. Notes about the data can be found at the link, or viewed with the command <code class="language-plaintext highlighter-rouge">print(sm.datasets.star98.NOTE)</code>. Let’s break down the data:</p> <ul> <li><code class="language-plaintext highlighter-rouge">NABOVE</code> and <code class="language-plaintext highlighter-rouge">NBELOW</code> are the binary response variable</li> <li><code class="language-plaintext highlighter-rouge">LOWINC</code> through <code class="language-plaintext highlighter-rouge">PCTYRRND</code> are the <strong>12</strong> independent variables, or regressors. These would be represented by \(\mathbf X_1,...,\mathbf X_p\) for \(p\) regressors.</li> <li>There are <strong>8</strong> interaction terms, representing non-linear interactions between two or more regressors. When an interaction is present, the effect of a regressor on the response variable depends on the value(s) of the variable(s) which it interacts with. The values of these interaction variables is simply the product of its interacting terms; for example, if variables \(A\) and \(B\) are interacting, the value of its interaction term, \(\mathbf X_{AB} = \mathbf X_A \circ \mathbf X_B\).</li> </ul> <p>Now, let’s load the data as a data frame:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre> 
 <span class="c1"># import packages for tutorial 
</span> <span class="kn">import</span> <span class="n">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>
 <span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
 <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">stats</span> 
 <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
 <span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
 <span class="kn">import</span> <span class="n">math</span>
 <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>
 
 <span class="c1"># load dataset as pandas dataframe
</span> <span class="n">data</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">star98</span><span class="p">.</span><span class="nf">load_pandas</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>In statsmodels, <code class="language-plaintext highlighter-rouge">endog</code> refers to the response variable(s). In this case, it will return the values of <code class="language-plaintext highlighter-rouge">NABOVE</code> and <code class="language-plaintext highlighter-rouge">NBELOW</code>. Running <code class="language-plaintext highlighter-rouge">data.endog.head()</code> should return a data frame that looks like this:</p> <table> <thead> <tr> <th>index</th> <th>NABOVE</th> <th>NBELOW</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>452.0</td> <td>355.0</td> </tr> <tr> <td>1</td> <td>144.0</td> <td>40.0</td> </tr> <tr> <td>2</td> <td>337.0</td> <td>234.0</td> </tr> <tr> <td>3</td> <td>395.0</td> <td>178.0</td> </tr> <tr> <td>4</td> <td>8.0</td> <td>57.0</td> </tr> </tbody> </table> <p><code class="language-plaintext highlighter-rouge">exog</code> refers to all of the other independent variables/regressors/interaction terms. Running <code class="language-plaintext highlighter-rouge">data.exog.head()</code> should return a data frame that contains the rest of the dsata.</p> <h2 id="visualize-the-data">Visualize the data</h2> <p>Let’s visualize the data and get a high-level idea of what’s going on. To start, let’s use seaborn to visualize the relationship between each of the independent variables (excluding the interaction terms) and the percentage of 9th grade students in each district scoring above the national median on the math section of the STAR test (100*<code class="language-plaintext highlighter-rouge">NABOVE</code>/(<code class="language-plaintext highlighter-rouge">NABOVE</code>+<code class="language-plaintext highlighter-rouge">NBELOW</code>)).</p> <p>It will be convenient to first reformat the data table into a form that is more friendly for exploratory data visualization. If you recall, each column represents a variable. We will reshape the table such that for each of the response variables (<code class="language-plaintext highlighter-rouge">id_vars</code>), there is one entry for each of the regressors (<code class="language-plaintext highlighter-rouge">value_vars</code>). By default, any columns not set as <code class="language-plaintext highlighter-rouge">id_vars</code> will be interpreted as <code class="language-plaintext highlighter-rouge">value_vars</code>. Each sample will have a record of the variable name and value. This process is called <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html"><strong>melting</strong></a> the dataframe.</p> <p>Here, we will subset the data table to the columns containing the binary response variables (<code class="language-plaintext highlighter-rouge">NABOVE</code> and <code class="language-plaintext highlighter-rouge">NBELOW</code>) and the first 12 variables (excluding the interaction terms). This corresponds to the first 14 columns of the dataframe.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre> <span class="n">plot_df</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">14</span><span class="p">].</span><span class="nf">melt</span><span class="p">(</span><span class="n">id_vars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">NABOVE</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">NBELOW</span><span class="sh">'</span><span class="p">])</span>
 <span class="n">plot_df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span> 
</pre></td></tr></tbody></table></code></pre></figure> <p>This should return a data frame that looks something like this: |index|NABOVE|NBELOW|variable|value| |—|—|—|—|—| |0|452.0|355.0|LOWINC|34.3973| |1|144.0|40.0|LOWINC|17.36507| |2|337.0|234.0|LOWINC|32.64324| |3|395.0|178.0|LOWINC|11.90953| |4|8.0|57.0|LOWINC|36.88889|</p> <p>Next, we add columns corresponding to the percentage of students above the national median (<code class="language-plaintext highlighter-rouge">PCTABOVE</code>) and the percentage of students below the national median (<code class="language-plaintext highlighter-rouge">PCTBELOW</code>).</p> <p>Then we will use <code class="language-plaintext highlighter-rouge">seaborn</code> to generate <a href="https://seaborn.pydata.org/generated/seaborn.regplot.html">line plots</a> relating <code class="language-plaintext highlighter-rouge">PCTABOVE</code> to each of the 12 independent variables selected.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre> <span class="c1"># calculate PCTABOVE and PCTBELOW
</span> <span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">PCTABOVE</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NABOVE</span><span class="sh">'</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NABOVE</span><span class="sh">'</span><span class="p">]</span><span class="o">+</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NBELOW</span><span class="sh">'</span><span class="p">])</span>
 <span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">PCTBELOW</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NBELOW</span><span class="sh">'</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NABOVE</span><span class="sh">'</span><span class="p">]</span><span class="o">+</span><span class="n">plot_df</span><span class="p">[</span><span class="sh">'</span><span class="s">NBELOW</span><span class="sh">'</span><span class="p">])</span>

 <span class="c1"># plot
</span> <span class="n">sns</span><span class="p">.</span><span class="nf">relplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="sh">'</span><span class="s">value</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sh">'</span><span class="s">PCTABOVE</span><span class="sh">'</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="sh">'</span><span class="s">variable</span><span class="sh">'</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="sh">'</span><span class="s">line</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">col</span> <span class="o">=</span> <span class="sh">'</span><span class="s">variable</span><span class="sh">'</span><span class="p">,</span> <span class="n">col_wrap</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">plot_df</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>This should give you a plot that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/glm/lineplots_regressors_vs_pctabove.png" sizes="95vw"/> <img src="/assets/img/glm/lineplots_regressors_vs_pctabove.png" class="img-fluid" width="700" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We can do the same thing but with <code class="language-plaintext highlighter-rouge">PCTBELOW</code> on the y-axis.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="c1"># plot
</span> <span class="n">sns</span><span class="p">.</span><span class="nf">relplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="sh">'</span><span class="s">value</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="sh">'</span><span class="s">PCTBELOW</span><span class="sh">'</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="sh">'</span><span class="s">variable</span><span class="sh">'</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="sh">'</span><span class="s">line</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">col</span> <span class="o">=</span> <span class="sh">'</span><span class="s">variable</span><span class="sh">'</span><span class="p">,</span> <span class="n">col_wrap</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">plot_df</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>This will give you a plot that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/glm/lineplots_regressors_vs_pctbelow.png" sizes="95vw"/> <img src="/assets/img/glm/lineplots_regressors_vs_pctbelow.png" class="img-fluid" width="700" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We see that the trends appear to be flipped, which is what we’d expect - variables that are positively correlated with the number of students scoring above the mean are most likely to be negatively correlated with the opposite outcome (number of students scoring below the mean).</p> <p>We can also take a look at hte distributions of the values of the independent variables (excluding interaction terms).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
 <span class="n">sns</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">variable</span><span class="sh">'</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">value</span><span class="sh">'</span><span class="p">,</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">12</span><span class="p">].</span><span class="nf">melt</span><span class="p">())</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Distribution of explanatory variables (without interactions)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>This will give you a plot that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/glm/regressors_distributions.png" sizes="95vw"/> <img src="/assets/img/glm/regressors_distributions.png" class="img-fluid" width="700" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="the-binomial-distribution"><a href="https://en.wikipedia.org/wiki/Binomial_distribution">The Binomial distribution</a></h1> <p>The response variable is binomially distributed - this means you have a discrete, non-negative number of “successes” and a discrete, non-negative number of “failures”. Here, this corresponds to <code class="language-plaintext highlighter-rouge">NABOVE</code> and <code class="language-plaintext highlighter-rouge">NBELOW</code>.</p> <h3 id="parameterization">Parameterization</h3> <p>The binomial distribution is parameterized by \(n\in \mathbb N\) and \(p\in [0,1]\), and is expressed as \(X\sim\text{Binomial}(n,p)\), where \(X\) is a binomial random variable.</p> <p>(Note: this is <em>not</em> the same \(p\) that we used earlier to define the dimensionality of each input vector \(\mathbf{x}_i ^T\)).</p> <h3 id="pmf">PMF</h3> <p>The probability mass function expresses the probability of getting \(k\) successes in \(n\) trials:</p> \[p(k,n,p)=\Pr(k;n,p)=\Pr(X=k)={n\choose k}p^k(1-p)^{n-k}\] <p>for \(k=0,1,2,...,n\) where \({n\choose k}\) is the binomial coefficient:</p> \[{n\choose k}=\frac{n!}{k!(n-k)!}\] <p>Essentially:</p> <ul> <li>\(k\) successes occur with probability \(p^k\), and</li> <li>\(n-k\) failures occur with probability \((1-p)^{n-k}\)</li> </ul> <p>Becasue the \(k\) successes can occur at any point within the \(n\) trials, there are \({n\choose k}\) different ways of observing \(k\) successes in \(n\) trials.</p> <h3 id="link-function-for-the-binomial-distribution---binary-logistic-regression">Link function for the Binomial distribution - <a href="https://newonlinecourses.science.psu.edu/stat504/node/216/">Binary Logistic Regression</a></h3> <p>The Binomial GLM can be expressed as:</p> \[y_i \sim \text{Binomial}(n_i,p_i)\] <p>with the logit link function:</p> \[p_i = \text{logit} ^{-1}(\mathbf X \beta) = \frac{\exp(\mathbf X \beta)}{1+\exp(\mathbf X \beta)} = \frac{1}{1+\exp(-\mathbf X \beta)}\] <h1 id="implement-the-glm">Implement the GLM</h1> <p>We need to estimate values of \(\beta\) corresponding to each of the variables in \(\mathbf X\), to give an estimate of the effect size of each predictor - that is, a measure of the effect that they have on the binomial response variable, which in this case is <code class="language-plaintext highlighter-rouge">NABOVE</code>.</p> <p>For each sample (district) in the dataset, we have \(k_i\) (<code class="language-plaintext highlighter-rouge">NABOVE</code>) and \(n_i\) (<code class="language-plaintext highlighter-rouge">NABOVE</code>+<code class="language-plaintext highlighter-rouge">NBELOW</code>). We can estimate \(p_i\) using the link function above, as we have the values of \(\mathbf X\) from the dataset in <code class="language-plaintext highlighter-rouge">data.exog</code>. The values of \(\beta\) giving the value of \(p_i\) that defines a distribution shape that best fits to the data - comprised of \(\mathbf X\) and \(k_i =\)<code class="language-plaintext highlighter-rouge">NABOVE</code> - are to be estimated by minimizing the log-likelihood function.</p> <h3 id="likelihood-function"><a href="https://en.wikipedia.org/wiki/Likelihood_function">Likelihood function</a></h3> <p>We will minimize the negative log-likelihood. Here, we will use the scipy implementation of <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html"><code class="language-plaintext highlighter-rouge">logpmf</code></a> and the parameters \(n,k,p\) to obtain the log-likelihood. The log-likelihood function can be defined as:</p> \[\log (\mathcal{L}(n,k,p\mid \mathbf{x}))=\sum_{i=1}^{N} \log (p(x_i\mid n,k,p)) = \sum_{i=1}^{N} \log (P(X=x_i\mid n,k,p))\] <p>where,</p> <ul> <li>\(n,k,p\) are the parameters defining the shape of the binomial distribution,</li> <li>\(N\) represents the total number of school districts (303),</li> <li>\(x_i\) represents <code class="language-plaintext highlighter-rouge">NABOVE</code> for district \(i\)</li> </ul> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre> <span class="c1"># define the negative log-likelihood function
</span> <span class="k">def</span> <span class="nf">loglik</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
     <span class="sh">'''</span><span class="s">betas = effect sizes of variables (parameters to estimate)
     x = values of variables
     y = NABOVE and NBELOW</span><span class="sh">'''</span>
    
     <span class="n">nTrials</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">NABOVE</span> <span class="o">+</span> <span class="n">y</span><span class="p">.</span><span class="n">NBELOW</span>
     <span class="n">pSuccess</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">betas</span><span class="p">)))</span><span class="o">**-</span><span class="mi">1</span>
     <span class="n">ll</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">binom</span><span class="p">.</span><span class="nf">logpmf</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">NABOVE</span><span class="p">,</span> 
                             <span class="n">n</span> <span class="o">=</span> <span class="n">nTrials</span><span class="p">,</span>
                             <span class="n">p</span> <span class="o">=</span> <span class="n">pSuccess</span><span class="p">)</span>
     <span class="k">return</span> <span class="o">-</span><span class="n">ll</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="fit-model">Fit model</h3> <p>For this example, we will restrict the number of variables to just the first 5 independent variables in the STAR98 dataset. This is because the optimizer can struggle with a large parameter space (a lot of \(\beta\) values).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre> <span class="c1"># fit model with first 5 variables interactions
</span> <span class="n">res_five</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">fun</span> <span class="o">=</span> <span class="n">loglik</span><span class="p">,</span>
                             <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span>
                             <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">5</span><span class="p">],</span> <span class="n">data</span><span class="p">.</span><span class="n">endog</span><span class="p">),</span>
                                       <span class="n">method</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Nelder-Mead</span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>We can see the results of the fitted model by running the command <code class="language-plaintext highlighter-rouge">res_five</code>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> final_simplex: (array([[-0.00694075,  0.034228  , -0.01203383, -0.00623375, -0.00491257],
       [-0.00694209,  0.03422786, -0.01202921, -0.0062293 , -0.00491935],
       [-0.00694485,  0.03422865, -0.01202725, -0.00622885, -0.00491606],
       [-0.00694175,  0.03422388, -0.01202843, -0.0062315 , -0.00491556],
       [-0.00694161,  0.03423066, -0.01202717, -0.00623275, -0.00491647],
       [-0.00694219,  0.03422552, -0.01202785, -0.00623148, -0.0049142 ]]), array([6787.56809261, 6787.56809815, 6787.56809911, 6787.56813934,
       6787.56814474, 6787.56815288]))
           fun: 6787.568092614503
       message: 'Optimization terminated successfully.'
          nfev: 471
           nit: 286
        status: 0
       success: True
             x: array([-0.00694075,  0.034228  , -0.01203383, -0.00623375, -0.00491257])
</code></pre></div></div> <p>We can print the coefficient estimates for the first five variables in our dataset with the following command:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">)[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">res_five</span><span class="p">.</span><span class="n">x</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">coeff for %s = %.3g</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Which should return something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>coeff for LOWINC = -0.00694
coeff for PERASIAN = 0.0342
coeff for PERBLACK = -0.012
coeff for PERHISP = -0.00623
coeff for PERMINTE = -0.00491

</code></pre></div></div> <p>Let’s see what happens if we fit the model with an additional variable - that means now we use the first six independent variables instead of the first five.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre> <span class="c1"># fit model with first 6 variables 
</span> <span class="n">res_six</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">fun</span> <span class="o">=</span> <span class="n">loglik</span><span class="p">,</span>
                             <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span>
                             <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">6</span><span class="p">],</span> <span class="n">data</span><span class="p">.</span><span class="n">endog</span><span class="p">),</span>
                                       <span class="n">method</span><span class="o">=</span> <span class="sh">'</span><span class="s">Nelder-Mead</span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should see something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>coeff for LOWINC = -0.0159
coeff for PERASIAN = 0.0161
coeff for PERBLACK = -0.0179
coeff for PERHISP = -0.0142
coeff for PERMINTE = -0.000361
coeff for AVYRSEXP = 0.0615
</code></pre></div></div> <p>The coefficient estimates have changed for some of these variables - how can we determine which estimates are more true to the data?</p> <h1 id="does-including-another-variable-improve-the-model-fit">Does including another variable improve the model fit?</h1> <p>We can use the <a href="https://en.wikipedia.org/wiki/Likelihood-ratio_test"><strong>likelihood-ratio test</strong></a> to determine whether the model with six variables fits the data better than the model with five variables.</p> <p>\(H_0\): The model with six variables does not fit the data significantly better than the model with five variables.</p> <p>\(H_A\): The model with six variables fits the data significantly better than the model with five variables.</p> <p>The likelihood ratio can be computed as:</p> \[LR = -2\ln{\left( \frac{\mathcal L(\theta_0)}{\mathcal L(\theta_A)} \right)} = -2\left(\ln{\left(\mathcal L(\theta_0)\right)}-\ln{\left(\mathcal L(\theta_A)\right)}\right)\] <p>Because \(LR\) is <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">\(\chi^2\)-distributed</a>, we can use this property to determine the p-value of the LRT:</p> \[p = 1-\text{CDF}_k(LR)\] <p>where \(k\) is the degrees of freedom, or the difference in the number of parameters for \(H_0\) and \(H_A\). In our example, \(k = 6-5 = 1\). To compute the p-value, we will use the scipy implementation of the survival function, <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html"><code class="language-plaintext highlighter-rouge">sf</code></a>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="c1"># define likelihood-ratio test
</span> <span class="k">def</span> <span class="nf">LRT</span><span class="p">(</span><span class="n">loglik_null</span><span class="p">,</span> <span class="n">loglik_alt</span><span class="p">,</span> <span class="n">nparams_null</span><span class="p">,</span> <span class="n">nparams_alt</span><span class="p">):</span>
     <span class="n">df</span> <span class="o">=</span> <span class="n">nparams_alt</span> <span class="o">-</span> <span class="n">nparams_null</span>
     <span class="n">lr</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">loglik_null</span> <span class="o">-</span> <span class="n">loglik_alt</span><span class="p">)</span>
     <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">chi2</span><span class="p">.</span><span class="nf">sf</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
     <span class="nf">return </span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Because we’ve defined the <code class="language-plaintext highlighter-rouge">LRT</code> function to take the log-likelihoods of \(H_0\) and \(H_A\), but our minimization function computed the <em>negative</em> log-likelihoods, we have to make sure to make this correction when passing those values to <code class="language-plaintext highlighter-rouge">LRT</code>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre> <span class="n">lr</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="nc">LRT</span><span class="p">(</span><span class="o">-</span><span class="n">res_five</span><span class="p">.</span><span class="n">fun</span><span class="p">,</span> <span class="o">-</span><span class="n">res_six</span><span class="p">.</span><span class="n">fun</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">LR = %.3g, p = %.3g</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should get a result that looks like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LR = 6.12e+03, p = 0
</code></pre></div></div> <p>The test appears to be very significant, indicating that we reject our null hypothesis that the model using six variables does not fit the data significantly better than the model using five variables.</p> <h1 id="compare-to-statsmodels-glm">Compare to statsmodels GLM</h1> <p>Let’s use the statsmodel library’s built in GLM function to fit the GLM using the same variables that we used in our implementation and compare the results:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="c1"># fit binom GLM with statsmodels using first six variables
</span> <span class="n">glm_binom</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nc">GLM</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">endog</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">6</span><span class="p">],</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="p">.</span><span class="n">families</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">())</span>
 <span class="n">res</span><span class="p">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">glm_binom</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
 <span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">full</span><span class="p">.</span><span class="nf">summary</span><span class="p">())</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should get something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                  Generalized Linear Model Regression Results                   
================================================================================
Dep. Variable:     ['NABOVE', 'NBELOW']   No. Observations:                  303
Model:                              GLM   Df Residuals:                      297
Model Family:                  Binomial   Df Model:                            5
Link Function:                    logit   Scale:                          1.0000
Method:                            IRLS   Log-Likelihood:                -3727.3
Date:                  Tue, 01 Oct 2019   Deviance:                       5536.2
Time:                          13:32:36   Pearson chi2:                 5.50e+03
No. Iterations:                       4   Covariance Type:             nonrobust
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
LOWINC        -0.0159      0.000    -43.482      0.000      -0.017      -0.015
PERASIAN       0.0161      0.001     32.095      0.000       0.015       0.017
PERBLACK      -0.0179      0.001    -27.228      0.000      -0.019      -0.017
PERHISP       -0.0142      0.000    -36.839      0.000      -0.015      -0.013
PERMINTE      -0.0004      0.001     -0.672      0.502      -0.001       0.001
AVYRSEXP       0.0615      0.001     76.890      0.000       0.060       0.063
==============================================================================
</code></pre></div></div> <p>We can see that the parameter estimates for the same six variables from <code class="language-plaintext highlighter-rouge">data.exog</code> are very similar to what we got with our implementation of the GLM.</p> <p>We can also use statsmodels to fit a GLM to the entire dataset and compare the coefficient estimates to what we got fitting a model to just some of the variables in the dataset.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="c1"># fit binom GLM with statsmodels 
</span> <span class="n">glm_binom</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nc">GLM</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">endog</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="p">.</span><span class="n">families</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">())</span>
 <span class="n">res</span><span class="p">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">glm_binom</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
 <span class="nf">print</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">full</span><span class="p">.</span><span class="nf">summary</span><span class="p">())</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should get something that looks like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                  Generalized Linear Model Regression Results                   
================================================================================
Dep. Variable:     ['NABOVE', 'NBELOW']   No. Observations:                  303
Model:                              GLM   Df Residuals:                      283
Model Family:                  Binomial   Df Model:                           19
Link Function:                    logit   Scale:                          1.0000
Method:                            IRLS   Log-Likelihood:                -3000.5
Date:                  Tue, 01 Oct 2019   Deviance:                       4082.4
Time:                          13:32:36   Pearson chi2:                 4.05e+03
No. Iterations:                       5   Covariance Type:             nonrobust
===========================================================================================
                              coef    std err          z      P&gt;|z|      [0.025      0.975]
-------------------------------------------------------------------------------------------
LOWINC                     -0.0169      0.000    -39.491      0.000      -0.018      -0.016
PERASIAN                    0.0101      0.001     16.832      0.000       0.009       0.011
PERBLACK                   -0.0186      0.001    -25.132      0.000      -0.020      -0.017
PERHISP                    -0.0142      0.000    -32.755      0.000      -0.015      -0.013
PERMINTE                    0.2778      0.027     10.154      0.000       0.224       0.331
AVYRSEXP                    0.2937      0.050      5.872      0.000       0.196       0.392
AVSALK                      0.0930      0.012      7.577      0.000       0.069       0.117
PERSPENK                   -1.4432      0.171     -8.461      0.000      -1.777      -1.109
PTRATIO                    -0.2347      0.032     -7.283      0.000      -0.298      -0.172
PCTAF                      -0.1179      0.019     -6.351      0.000      -0.154      -0.081
PCTCHRT                     0.0047      0.001      3.748      0.000       0.002       0.007
PCTYRRND                   -0.0036      0.000    -16.239      0.000      -0.004      -0.003
PERMINTE_AVYRSEXP          -0.0157      0.002     -9.142      0.000      -0.019      -0.012
PERMINTE_AVSAL             -0.0044      0.000    -10.137      0.000      -0.005      -0.004
AVYRSEXP_AVSAL             -0.0048      0.001     -5.646      0.000      -0.006      -0.003
PERSPEN_PTRATIO             0.0686      0.008      8.587      0.000       0.053       0.084
PERSPEN_PCTAF               0.0372      0.004      9.099      0.000       0.029       0.045
PTRATIO_PCTAF               0.0057      0.001      6.760      0.000       0.004       0.007
PERMINTE_AVYRSEXP_AVSAL     0.0002   2.68e-05      9.237      0.000       0.000       0.000
PERSPEN_PTRATIO_PCTAF      -0.0017      0.000     -8.680      0.000      -0.002      -0.001
===========================================================================================
</code></pre></div></div> <p>You can see that some of the coefficient estimates differ significantly from our estimates - this is due to the fact that including more variables in the model changes the coefficient estimates.</p>]]></content><author><name></name></author><category term="tutorials"/><category term="code"/><category term="machine_learning"/><category term="glm"/><category term="modeling"/><category term="statistics"/><summary type="html"><![CDATA[A Python-coded implementation of binomial generalized linear model]]></summary></entry><entry><title type="html">Ordinary least squares</title><link href="https://zrcjessica.github.io/blog/2023/ols/" rel="alternate" type="text/html" title="Ordinary least squares"/><published>2023-02-23T16:00:00+00:00</published><updated>2023-02-23T16:00:00+00:00</updated><id>https://zrcjessica.github.io/blog/2023/ols</id><content type="html" xml:base="https://zrcjessica.github.io/blog/2023/ols/"><![CDATA[<p>Ordinary least squares (OLS) is a method for fitting a linear regression model using least squares - that is, minimizing the sum of the squared differences between the true vs. predicted outputs of the linear model to determine the weights that optimize the model fit. The Jupyter notebook for this tutorial can be found on my <a href="https://github.com/zrcjessica/ml_concepts/blob/main/OLS_tutorial.ipynb">GitHub</a>.</p> <h1 id="simple-linear-regression-model">Simple linear regression model</h1> <p>A linear regression model for a sample \(i\) can be formulated as:</p> \[y_i = \alpha + \beta x_i + \varepsilon _i\] <p>where \((\alpha,\beta)\) are the model parameters, \(x_i\) is a scalar predictor variable, and \(y_i\) is a response variable. \(\varepsilon_i\) is a sample specific error term.</p> <h1 id="the-goal-of-ols">The goal of OLS</h1> <p>The goal of OLS is to estimate the values of \((\alpha, \beta)\) in the model given a set of training data so as to make predictions of \(\hat{y}_i=\alpha + \beta x_i\). In order to estimate the parameter values, OLS mimizes the <strong>sum of squared errors</strong> between the observed vs. predicted response variables for each predictor $x_i$:</p> \[SSE = \sum _{i=1} ^N (y_i - \hat{y}_i)^2\] <p>In the case of simple linear regression, we can estimate the parameters as follows:</p> \[\beta = \frac{\sum_{i=1}^{N} \left(x_i-\bar{\mathbf{x}} \right) \left(y_i-\bar{\mathbf{y}} \right)}{\sum _{i=1}^{N} \left( x_i - \bar{\mathbf{x}} \right)^2} = \frac{Cov(x,y)}{Var(x)}\] \[\alpha = \bar{\mathbf{y}} - \beta \bar{\mathbf{x}}\] <h1 id="simulate-data">Simulate data</h1> <p>Let’s start by simulating some data for this tutorial. We will randomly generate 100 values for each \(\mathbf{y}\) and \(\mathbf{x}\). We will also generate a residual for each value to simulate some noise (\(\varepsilon\)) in the linear relationship between \(\mathbf{x}\) and \(\mathbf{y}\). We will use predetermined values of \(\alpha = 2\) and \(\beta = 0.3\) for generating these values.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
 
 <span class="c1"># random seed
</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
 
 <span class="c1"># define alpha and beta
</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">2</span>
 <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.3</span>
 
 <span class="c1"># generate x, y
</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span>   <span class="c1"># generate 100 values of x with mean = 2, stddev = 3
</span> <span class="n">res</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>   <span class="c1"># Generate 100 error terms
</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">res</span>   <span class="c1"># calculate y</span>
</pre></td></tr></tbody></table></code></pre></figure> <h1 id="estimate-hatalpha-and-hatbeta">Estimate \(\hat{\alpha}\) and \(\hat{\beta}\)</h1> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre> <span class="c1"># calculate mean of x and y
</span> <span class="n">xbar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="n">ybar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

 <span class="c1"># estimate beta and alpha from data 
</span> <span class="n">betaHat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">xbar</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">ybar</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">xbar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
 <span class="n">alphaHat</span> <span class="o">=</span> <span class="n">ybar</span> <span class="o">-</span> <span class="n">betaHat</span><span class="o">*</span><span class="n">xbar</span>
 
 <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">betaHat = %.5f, alphaHat = %.5f</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">betaHat</span><span class="p">,</span> <span class="n">alphaHat</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Check and see how well these estimated values of \(\hat{\alpha}\) and \(\hat{\beta}\) match up against the values of \(\alpha=2\) and \(\beta=0.3\) that we used to generate this simulated dataset!</p> <h1 id="predict-hatmathbfy">Predict \(\hat{\mathbf{y}}\)</h1> <p>Now that we have fitted our simple linear regression model, we can use the estimated parameters to make estimates of \(\hat{\mathbf{y}}\) given the predictors \(\mathbf{x}\).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre> <span class="n">yhat</span> <span class="o">=</span> <span class="n">alphaHat</span> <span class="o">+</span> <span class="n">betaHat</span><span class="o">*</span><span class="n">x</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Let’s plot the estimates of \(\hat{\mathbf{y}}\) against the true response variables \(\mathbf{y}\):</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>   <span class="c1"># scatter plot showing actual data
</span> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span>     <span class="c1"># regression line
</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Predicted vs. Actual values</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)</span>

 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>It should look something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ols/true_vs_pred_scatterplot.png" sizes="95vw"/> <img src="/assets/img/ols/true_vs_pred_scatterplot.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We can also check the Pearson correlation between \(\hat{\mathbf{y}}\) and \(\mathbf{y}\):</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">pearsonr</span>

 <span class="n">corr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">pearsonr</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Pearson</span><span class="sh">'</span><span class="s">s r = %.3f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">corr</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="tutorials"/><category term="code"/><category term="machine_learning"/><category term="ols"/><category term="regression"/><category term="modeling"/><category term="statistics"/><summary type="html"><![CDATA[A Python-coded implementation of OLS for fitting a linear regression model to simulated data]]></summary></entry><entry><title type="html">Maximum likelihood estimation</title><link href="https://zrcjessica.github.io/blog/2023/mle/" rel="alternate" type="text/html" title="Maximum likelihood estimation"/><published>2023-02-21T14:12:00+00:00</published><updated>2023-02-21T14:12:00+00:00</updated><id>https://zrcjessica.github.io/blog/2023/mle</id><content type="html" xml:base="https://zrcjessica.github.io/blog/2023/mle/"><![CDATA[<p>Maximum likelihood estimation (MLE) is a method used for estimating the parameters used to define a probability distribution by fitting to observed data drawn from said distribution. As the name implies, this method involves maximizing the <em>likelihood</em> that the observed data comes from a statistical model. A silly analogy I sometimes use to explain MLE involves finding clothes that fit. Here, the observed data is someone who’s trying to find clothes that fit. MLE is like the process of trying on clothing items of different sizes until we find the sizes that fit the best. The clothing sizes are analogous to the parameters that define the shape of statistical distribution from which the observed data came (so the distribution itself is equivalent to the piece of clothing). Assuming the individual has no idea what their size is (this is actually somewhat realistic for womens fashion, where sizing can be wildly inconsistent across brands), MLE will start with a set of guesses at what the best sizes could be and then try different sizes to improve the fit over each iteration. In this tutorial, we will simulate data and use MLE to recover the parameters used to simulate the data. A Jupyter notebook for this tutorial can be found on my <a href="https://github.com/zrcjessica/ml_concepts/blob/main/MLE_tutorial.ipynb">GitHub</a>.</p> <h1 id="mle-for-normally-distributed-data">MLE for normally distributed data</h1> <p>Let’s begin by walking through an example using MLE to estimate the parameters for a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> from which we draw a sample set of observed data. To begin, let’s start by drawing $N=1000$ data points from a normal distribution defined as \(N(\mu=4,\sigma=3)\). We will refer to the sampled data as \(\mathbf{x} = x_1, x_2,...,x_N\).</p> <h2 id="simulate-data">Simulate data</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="code"><pre> <span class="c1"># load the packages we'll need for this tutorial
</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>
 <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">stats</span> 
 <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
 <span class="kn">import</span> <span class="n">math</span>
 <span class="kn">from</span> <span class="n">scipy.special</span> <span class="kn">import</span> <span class="n">factorial</span>
 
 <span class="c1"># define parameters
</span> <span class="n">MU</span> <span class="o">=</span> <span class="mi">4</span>
 <span class="n">SIGMA</span> <span class="o">=</span> <span class="mi">3</span>
 <span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
 
 <span class="c1"># add some noise
</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
 
 <span class="c1"># sample data from normal distribution
</span> <span class="n">norm_sample</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">rvs</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">MU</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">SIGMA</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">N</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="visualize">Visualize</h3> <p>Let’s take a look at what our sample data looks like.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">norm_sample</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">distribution of Gaussian random sample</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should get something that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mle/guassian_sample.png" sizes="95vw"/> <img src="/assets/img/mle/guassian_sample.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="define-likelihood-function">Define likelihood function</h2> <p>In MLE, we want to optimize a <strong>likelihood function</strong>, which measures the likelihood that a given set of data came from a statistical distribution defined by a current set of parameters. The <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">likelihood function (\(\mathcal{L}\) for a continuous distribution</a> (our data is continuous) is defined as:</p> \[\mathcal{L}(\theta,\mathbf{x}) = f_{\theta}(\mathbf{x})=P_{\theta}(X=\mathbf{x})\] <p>Here, \(\theta={\theta_1,...,\theta_M}\) represents the set of \(M\) parameters defining the distribution that we are trying to fit the data to. \(f_{\theta}(\mathbf{x})=P_{\theta}(X=\mathbf{x})\) is just another way of writing the probability density function (PDF) for the continuous distribution given the parameterization \(\theta\). In other words, it means “what is the probability that the data we are observing came from the statistical distribution defined the parameters \(\theta\)?” Now because in MLE we are typically looking at a set of data points, we are actually trying to find the <em>joint probability</em> that all of the data points in our sample came from the same distribution defined by a given set of parameters \(\theta\). Because the joint probability of <em>identical and independently distributed</em> variables is simply their product, our likelihood function can therefore be expressed as:</p> \[\mathcal{L}(\theta,\mathbf{x}) = \prod _{i=1} ^{N} f_{\theta}(\mathbf{x})= \prod _{i=1} ^{N} P_{\theta}(X=\mathbf{x})\] <p>In our example, we will try to fit the data back to a normal distribution. Therefore, \(\theta=[\mu,\sigma]\). Even if we didn’t have the prior knowledge that we sampled our data from a normal distribution, observing the distribution of the data clearly indicates that the data follows a normal distribution, making it a good distribution to fit our data to. The PDF for the normal distribution is defined as:</p> \[f_{\mu,\sigma}(x) = f(x|\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp { \left( - \frac{(x-\mu)^2}{2\sigma^2} \right) }\] <p>This is the function that we will be trying to maximize. However, in practice it is more common to optimize over the <strong>log-likelihood</strong> function (LLF), because the product of many probability values can lead to floating point errors in computation. The LLF can be expressed as:</p> \[\log{\left( \mathcal{L}(\theta | \mathbf{x}) \right)} = \sum _{i=1} ^N \log{ \left( f_{\theta}(x_i) \right)} = \sum _{i=1} ^N \log{(P_{\theta})}(X=x_i)\] <h2 id="optimize">Optimize</h2> <p>Many built-in optimizers don’t have a maximization function, just a minimization function. Therefore, instead of maximizing the log-likelihood, we will <strong>minimize the negative log-likelihood</strong>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre> <span class="c1"># define neg llf
</span> <span class="k">def</span> <span class="nf">ll_norm</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
     <span class="sh">'''</span><span class="s">
     params is a list of parameters to estimate: [mu, sigma]
     x is list of normally distributed values described by estimated params
     </span><span class="sh">'''</span>
    
     <span class="n">mu</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
     <span class="n">sigma</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
     <span class="n">loglik</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
     <span class="k">return</span> <span class="o">-</span><span class="n">loglik</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Here’s a quick intro to essential arguments for the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"><code class="language-plaintext highlighter-rouge">scipy.optimize.minimize</code></a> function: -<code class="language-plaintext highlighter-rouge">fun</code>: function to minimize - for us this is the negative log-likelihood function</p> <ul> <li><code class="language-plaintext highlighter-rouge">x0</code>: initia guesses for the parameters we’re estimating, in the form of a list, tuple, or numpy array</li> <li><code class="language-plaintext highlighter-rouge">args</code>: any other variables to pass to the function <code class="language-plaintext highlighter-rouge">fun</code>, given in a list, tuple, or numpy array</li> <li><code class="language-plaintext highlighter-rouge">bounds</code>: bounds for parameters to be estimated, given as a list of lists or tuple of tuples, corresponds to params in <code class="language-plaintext highlighter-rouge">x0</code></li> </ul> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="c1"># minimize negative log-likelihood
</span> <span class="n">norm_res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">fun</span> <span class="o">=</span> <span class="n">ll_norm</span><span class="p">,</span>
               <span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
               <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">norm_sample</span><span class="p">),</span>
             <span class="n">bounds</span> <span class="o">=</span> <span class="p">((</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">),(</span><span class="mf">1e-6</span><span class="p">,</span><span class="bp">None</span><span class="p">)))</span>

 <span class="n">norm_res</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The output should look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      fun: 2509.8651698184385
 hess_inv: &lt;2x2 LbfgsInvHessProduct with dtype=float64&gt;
      jac: array([ 0.00040927, -0.00027285])
  message: b'CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH'
     nfev: 45
      nit: 14
   status: 0
  success: True
        x: array([4.02069038, 2.97703045])

</code></pre></div></div> <p>Here’s a brief overview of what these outputs mean:</p> <ul> <li><code class="language-plaintext highlighter-rouge">fun</code>: minimum value of function at estimated parameters</li> <li><code class="language-plaintext highlighter-rouge">nfev</code>: number of function evaluations</li> <li><code class="language-plaintext highlighter-rouge">nit</code>: number of iterations</li> <li><code class="language-plaintext highlighter-rouge">success</code>: bool - did the optimizer run into an issue?</li> <li><code class="language-plaintext highlighter-rouge">x</code>: array of estimated parameters that minimize the function, corresponding to <code class="language-plaintext highlighter-rouge">x0</code></li> </ul> <p>We can print the final parameter estimates from MLE and see how they compare to the values we used to simulate the data that we showed the algorithm:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre> <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">mu = %d, sigma = %d</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">MU</span><span class="p">,</span> <span class="n">SIGMA</span><span class="p">))</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">mu_est = %.4f, sigma_est = %.4f</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">norm_res</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">norm_res</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should see that the estimated parameters \(\hat{\mu}\) and \(\hat{\sigma}\) are quite close to the true parameter values that we used to define the normal distribution from which we simulated the data points. If you refer to my <a href="https://github.com/zrcjessica/ml_concepts/blob/main/MLE_tutorial.ipynb">Jupyter notebook</a>, you can also see how changing the size of the data that we give to the MLE algorithm change the resulting parameter estimates.</p> <h1 id="mle-for-poisson-distributed-data">MLE for Poisson-distributed data</h1> <p>Now let’s see how we can use MLE to estimate the parameters of the distribution from which a set of <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distributed data</a> came from. Poisson-distributed data is discrete and nonnegative. The Poisson distribution takes only one parameter, $\lambda&gt;0$, which represents the rate at which events occur.</p> <p>It is a discrete probability distribution that expresses the probability of a given number of events, \(k\), occurring in a fixed interval of time or space, if the events occur with a known constant rate \(\lambda\) and independently of the time since the last event.</p> <p>Here, we will randomly generate a set of Poisson distributed data for a specified value of \(\lambda\).</p> <p>Note: The <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html">scipy notation for the Poisson distribution</a> uses \(\mu\) in place of \(\lambda\).</p> <h2 id="simulate-data-1">Simulate data</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre> <span class="c1"># define parameters
</span> <span class="n">LAMBDA</span> <span class="o">=</span> <span class="mf">0.5</span>
 <span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
 
 <span class="c1"># sample data from poisson distribution
</span> <span class="n">pois_sample</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">poisson</span><span class="p">.</span><span class="nf">rvs</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">MU</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">SIGMA</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">N</span><span class="p">)</span>  
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="visualize-1">Visualize</h3> <p>Let’s take a look at what our sample data looks like.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">pois_sample</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">kpois_sample</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">distribution of Poisson random sample</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should get something that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mle/pois_sample.png" sizes="95vw"/> <img src="/assets/img/mle/pois_sample.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="define-likelihood-function-1">Define likelihood function</h2> <p>For a <a href="https://en.wikipedia.org/wiki/Likelihood_function#Discrete_probability_distribution">discrete distribution</a> such as the Poisson, the likelihood function doesn’t change much, except it takes the products of the probability mass functions (PMF) rather than the probability distribution function (PDF):</p> \[\mathcal{L}(\theta\mid \mathbf{x})=p_{\theta}(\mathbf{x})=P_{\theta}(X=\mathbf{x})\] <p>or,</p> \[\mathcal{L}(\theta\mid \mathbf{x})=\prod_{i=1}^{N} p_{\theta}(x_i) = \prod_{i=1}^{N} P_{\theta}(X=x_i)\] <p>Again, we will need to define the <a href="https://en.wikipedia.org/wiki/Poisson_distribution#Probability_of_events_for_a_Poisson_distribution">PMF for the Poisson distribution</a> to define our likelihood function. Because the Poisson is parameterized by \(\lambda\), \(\theta=[\lambda]\) for our likelihood function.</p> \[p_{\lambda}(k)=p(k \mid \lambda)=\exp^{-\lambda}\frac{\lambda^k}{k!}\] <p>Again, we will minimize over the negative LLF. The LLF for the Poisson is expressed as:</p> \[\log (\mathcal{L}(\theta\mid \mathbf{k}))=\sum_{i=1}^{N} \log (p_{\theta}(k_i)) = \sum_{i=1}^{N} \log (P_{\theta}(X=k_i))\] <h2 id="optimize-1">Optimize</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">ll_pois</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
     <span class="sh">'''</span><span class="s">params is list of parameters to estimate: [lambda]
     k is list of Poisson distributed values described by estimated parameter</span><span class="sh">'''</span>
    
     <span class="n">lmbd</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
     <span class="n">loglik</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lmbd</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">lmbd</span><span class="o">**</span><span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="nf">factorial</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
     <span class="k">return</span> <span class="o">-</span><span class="n">loglik</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Note: When you want to set bounds but you only have one parameter to estimate, you need to format it as demonstrated below, or you will run into an error.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre> <span class="n">pois_res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">fun</span> <span class="o">=</span> <span class="n">ll_pois</span><span class="p">,</span>
               <span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-6</span><span class="p">],</span>
               <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">poisson_sample</span><span class="p">),</span>
             <span class="n">bounds</span> <span class="o">=</span> <span class="p">((</span><span class="mf">1e-6</span><span class="p">,</span><span class="bp">None</span><span class="p">),))</span>

 <span class="n">pois_res</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The output should look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      fun: 937.3870827020155
 hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;
      jac: array([-0.0001819])
  message: b'CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH'
     nfev: 56
      nit: 17
   status: 0
  success: True
        x: array([0.51099991])
</code></pre></div></div> <p>We can print the coefficients:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre> <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">lambda = %.1f</span><span class="sh">'</span> <span class="o">%</span> <span class="n">LAMBDA</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">lambda_est = %.4f</span><span class="sh">'</span> <span class="o">%</span> <span class="n">pois_res</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>scipy has built-in pdf and logpdf functions for a number of statistical distributions, so in practice you could use those functions instead of implementing them yourself.</p>]]></content><author><name></name></author><category term="tutorials"/><category term="code"/><category term="statistics"/><category term="mle"/><category term="machine_learning"/><category term="code"/><category term="jupyter"/><summary type="html"><![CDATA[A Python-coded implementation of maximum likelihood estimation for recovering the parameters used to define statistical distributions by fitting to values sampled from the distribution]]></summary></entry><entry><title type="html">Gradient descent</title><link href="https://zrcjessica.github.io/blog/2023/gradient-descent/" rel="alternate" type="text/html" title="Gradient descent"/><published>2023-02-21T13:41:00+00:00</published><updated>2023-02-21T13:41:00+00:00</updated><id>https://zrcjessica.github.io/blog/2023/gradient-descent</id><content type="html" xml:base="https://zrcjessica.github.io/blog/2023/gradient-descent/"><![CDATA[<p>Gradient descent is an optimization algorithm used to find the optimal weights for a model. While it is popularly used to tune the weights in neural networks, in this tutorial we will be using it to try to recover the coefficients used to simulate a toy dataset. We will also demonstrate the differences between batch, stochastic, and mini-batch gradient descent and defining some of the relevant terms. Here is the link to the Jupyter notebook for this tutorial on my <a href="https://github.com/zrcjessica/ml_concepts/blob/main/gradient_descent.ipynb">GitHub</a>.</p> <h1 id="simulating-the-data">Simulating the data</h1> <p>We will begin by simulating the data for this example. Our simulated dataset will contain \(n=10000\) samples using a linear function defined as:</p> \[f(x_1, x_2, x_3) = ax_1 + bx_2 + cx_3 + \varepsilon\] <p>Here, \({x_1,x_2,x_3}\) represent an input vector of size 3 and \(\varepsilon\) is noise.</p> <p>To do this, we will first simulate the coefficients of the model, \(a,b,c\) by picking 3 random integers between 1 and 10.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
 
 <span class="c1"># simulate coeffs 
</span> <span class="n">coeffs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
 
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Coefficients: a = %d, b = %d, c = %d</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">coeffs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">coeffs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">coeffs</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Next, we will generate the inputs and the noise for each sample by randomly sampling from the standard normal distribution, and then calculate the outputs based on the equation defined above.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre> <span class="c1"># define number of data points
</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>

 <span class="c1"># define inputs in dataset
</span> <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>

 <span class="c1"># define noise
</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

 <span class="c1"># get outputs for dataset
</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X</span><span class="o">*</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Just to get a better idea of what the data looks like, let’s coerce the dataset we’ve simulated into a data frame format and take a look at it.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre> <span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

 <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)))),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">x1</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">x2</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">x3</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Now let’s cover the concepts that will be important for this tutorial.</p> <h1 id="gradient-descent">Gradient descent</h1> <p>Gradient descent is an optimization method that is popularly used in machine learning to find the best parameters (more often referred to as <em>weights</em> in the case of neural networks) for a given model. It works quite like how it sounds - by following gradients to descend towards the minimum of a <strong>cost function</strong>. Cost functions are a function of the difference between the true and predicted outputs of a given model. There are a number of different cost functions out there. For example, cross entropy cost functions are popularly used for classification problems while squared error cost functions are popular used for regression problems. Conceptually, gradient descent looks something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/gradient-descent.jpeg" sizes="95vw"/> <img src="/assets/img/gradient_descent/gradient-descent.jpeg" class="img-fluid" width="600" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/">Image source</a></p> <p>The y-axis represents the possible values of the cost function, or error, evaluated for different model weights \(\mathbf{w} = \{w_1, w_2,...,w_j\}\) where \(j\) is the total number of weights in the model. In the beginning of the model training process, we have random weight values which will yield different errors. At each iteration of training, we will calculate the gradient at the point in the cost function given the current weights, multiply the gradient by a pre-determined <strong>learning rate</strong>, and then descend along the cost function curve accordingly.</p> <h1 id="types-of-gradient-descent">Types of gradient descent</h1> <p>There are a couple of different types of gradient descent out there. In order to understand the differences between these, it is first helpful to define some of the following terms:</p> <ul> <li><strong>sample</strong>: a sample is a single data point that can be passed to the model</li> <li><strong>batch</strong>: a hyperparameter which defines the number of samples that the model must evaluate before updating the weights in the model</li> <li><strong>epoch</strong>: a hyperparameter that defines the number of time that the entire training dataset will be passed through the model</li> </ul> <p>Now, we can start talking about the different types of gradient descent!</p> <h2 id="batch-gradient-descent">Batch gradient descent</h2> <p>In batch gradient descent, every data point in our dataset is evaluated in a given training iteration and the gradients are summed for each data point and then used to make a single update to the weights in the model. In other words, <strong>the size of a batch is equivalent to the size of the entire training data set</strong>, and the number of batches in an epoch is 1.</p> <h5 id="pros">Pros</h5> <ul> <li>Because we only update the model after evaluating all data points, this results in fewer updates and a more computationally efficient training process</li> <li>Fewer udpates results in a more stable gradient and more stable convergence</li> </ul> <h5 id="cons">Cons</h5> <ul> <li>The more stable gradient may result in the model converging earlier to a less optimal set of parameters, e.g. a local minimum instead of a global minimum</li> <li>Prediction errors must be accumulated across all training samples because the model weights are updated after evaluation of all samples</li> <li>Usually the entire dataset needs to be loaded in memory for the model to work with it</li> <li>Altogether these cons make this approach slower</li> </ul> <h2 id="stochastic-gradient-descent-sgd">Stochastic gradient descent (SGD)</h2> <p>In stochastic gradient descent, a random subset of the training dataset is evaluated in each training iteration to provide a single update to the weights in the model. Typically, SGD refers to a random subset size of 1; that is, <strong>each batch consists of a single sample</strong>. The number of batches in a single epoch, then, is equivalent to the number of samples in the entire training data set.</p> <h5 id="pros-1">Pros</h5> <ul> <li>Because the model weights are updated more frequently, we can have a higher resolution of the how the model performs and how quickly it’s learning</li> <li>The higher frequency of model updates may help the model learn faster for some problems</li> <li>The “noisier” model updates may help the model avoid local minima</li> </ul> <h5 id="cons-1">Cons</h5> <ul> <li>Updating the model more frequently is more computational expensive</li> <li>The more frequent model updates result in noisier gradients, resulting in more variance in the error landscape across training epochs</li> <li>The noisier updates can also make it harder for the algorithm to optimize</li> </ul> <h2 id="stochastic-mini-batch-gradient-descent">(Stochastic) mini-batch gradient descent</h2> <p>As with SGD, we are picking random subsets of the data to pass through the model in order to inform the updating of the weights; however, here <strong>the batch size is somewhere between a single sample and the entirety of the training data set</strong>. Therefore, in a single epoch we see a number of samples roughly equivalent to the total number of samples in the training dataset divided by the batch size. This is very popular for training neural networks.</p> <h5 id="pros-2">Pros</h5> <ul> <li>More frequency model updates than batch gradient descent allows for a more robust convergence and a better likelihood of avoiding local minima</li> <li>Less frequent model updates than SGD results in greater computational efficiency</li> <li>Smaller batches means that we don’t have to have the entire training dataset in memory (as with batch gradient descent)</li> </ul> <h5 id="cons-2">Cons</h5> <ul> <li>We have to define an additional batch size hyperparameter</li> </ul> <h1 id="cost-function">Cost function</h1> <p>I will be using two closely related terms in this section: <strong>loss function</strong> and <strong>cost function</strong>. A <strong>loss function</strong> is calculated for a single data point while a <strong>cost function</strong> is the sum of the <strong>losses</strong> across all the points in a batch. For our tutorial, we will use the <strong>Least Squared Error</strong> loss function, which is commonly used for linear regression. For a given sample \(j\), it is defined as:</p> \[LSE = \frac{1}{2}(\hat{y}_j - y_j)^2\] <p>where \(\hat{y}_i\) is the predicted output for a given input vector \(\mathbf{x_i}\). Its associated cost function is known as <strong>Mean Squared Error (MSE)</strong>:</p> \[MSE = \frac{1}{2m} \sum _{j=1} ^{m} (\hat{y}_j - y_j)^2\] <p>where \(m\) is the total number of training samples.</p> <p>In our example, \(\hat{y}_j\) is calculated as a linear function that is essentially the dot product between the weights in the model, \(\mathbf{w}\) and the inputs for a given sample, \(\mathbf{x}_j\):</p> \[\hat{y}_j = w_1 x_{i_1} + w_2 x_{i_2} + w_3 x_{i_3}\] <p>Thus, the cost function can be expression as a function of \(\mathbf{w}\):</p> \[J(\mathbf{w}) = \frac{1}{2m} \sum _{j=1} ^{m} ((w_1 x_{j_1} + w_2 x_{j_2} + w_3 x_{j_3}) - y_j)^2\] <p>By training on the data we simulated using the coefficients \(a, b, c\), we are trying to optimize the weights \(w_1, w_2, w_3\) to try to recover the coefficients used to generate the data.</p> <p>Here, let’s define the functions we’ll use for calculating \(\hat{y}_j\) and MSE:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">pred</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
     <span class="sh">"""</span><span class="s">
     predict y given inputs and weights
     </span><span class="sh">"""</span>
     <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
     <span class="k">return</span> <span class="n">y_pred</span>


 <span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
     <span class="sh">"""</span><span class="s">
     calculate MSE 
     </span><span class="sh">"""</span>
     <span class="n">mse</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">))</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">))</span>
     <span class="k">return</span> <span class="n">mse</span>
</pre></td></tr></tbody></table></code></pre></figure> <h1 id="learning-rate">Learning rate</h1> <p>The learning rate is a small value that determines how far along the curve we move to update the weights. It’s important to pick the right learning rate - a large learning rate can result in overshotting the optimum value, while a small learning rate will make it take much longer for the model to converge on the optimum.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/learning_rate.jpeg" sizes="95vw"/> <img src="/assets/img/gradient_descent/learning_rate.jpeg" class="img-fluid" width="600" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/">Image source</a></p> <p>The learning rate will be denoted by \(\eta\). In our tutorial, let’s assign a learning rate of \(\eta = 0.01\).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="c1"># learning rate
</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
</pre></td></tr></tbody></table></code></pre></figure> <h1 id="gradient-descent-algorithm">Gradient descent algorithm</h1> <p>In each iteration of the training algorithm, we will update each weight in our model \(w_i \rightarrow w_i'\) with the following formula:</p> \[w_i' = w_i - \eta \frac{\partial{J}}{\partial{w_i}}\] <p>where</p> \[\frac{\partial{J}}{\partial{w_i}} = \frac{1}{m} \sum _{j=1} ^{m} ((w_1 x_{j_1} + w_2 x_{j_2} + w_3 x_{j_3}) - y_i)(x_{j_i})\] <p>Let’s define the function for calculating the gradient of the cost function:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">x_i</span><span class="p">):</span>
     <span class="sh">"""</span><span class="s">
     calculate the partial derivative of cost function wrt w_i
     </span><span class="sh">"""</span>
     <span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span><span class="o">*</span><span class="n">x_i</span><span class="p">)</span>
     <span class="k">return</span> <span class="n">grad</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Now let’s begin implementing our code! We’ll start with an example of <strong>batch gradient descent</strong>.</p> <h1 id="example---batch-gradient-descent">Example - batch gradient descent</h1> <p>In batch gradient descent, each epoch contains only one batch, and the batch size is equivalent to the entire size of the training dataset. We will define 1000 epochs.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre></td><td class="code"><pre> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>
 <span class="n">m</span> <span class="o">=</span> <span class="n">n</span>
 
 <span class="c1"># initialize random weights to start
</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">initial weights:</span><span class="sh">"</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>

 <span class="c1"># collect value of cost function at each iter
</span> <span class="n">cost_list</span> <span class="o">=</span> <span class="p">[]</span>

 <span class="c1"># collect the weights at the end of each epoch
</span> <span class="n">weights_updates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
 <span class="n">weights_updates</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
 <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">epoch = %d</span><span class="sh">'</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
          
     <span class="c1"># predict on training set 
</span>     <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">apply_along_axis</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arr</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">)</span>

     <span class="c1"># calculate cost
</span>     <span class="n">cost</span> <span class="o">=</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
     <span class="n">cost_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

     <span class="c1"># update the weights
</span>     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
         <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="nf">grad</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>

     <span class="nf">print</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
     <span class="n">weights_updates</span><span class="p">[</span><span class="nb">iter</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

 <span class="c1"># calculate time
</span> <span class="n">batch_gd_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Let’s visualize the MSE over each training iteration:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">cost_list</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Cost</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">MSE over training epochs - batch gradient descent</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should see something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/mse_bgd.png" sizes="95vw"/> <img src="/assets/img/gradient_descent/mse_bgd.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Let’s also visualize how the weights changed at the end of each training epoch:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w1</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w2</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w3</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">weights</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epochs</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Weights over training epochs - batch gradient descent</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should see something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/weights_bgd.png" sizes="95vw"/> <img src="/assets/img/gradient_descent/weights_bgd.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now let’s examine <code class="language-plaintext highlighter-rouge">weights</code>. It should be a numpy array that contains values that look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([1.00021995, 1.00020721, 2.02081245])
</code></pre></div></div> <p>Out of curiosity, let’s run linear regression on our data and see how the coefficients obtained with the regression compare to what we got with our implementation of gradient descent.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

 <span class="n">reg</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
 <span class="n">reg</span><span class="p">.</span><span class="n">coef_</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The resulting coefficients probably look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([1.00032104, 1.00020243, 2.02069302])
</code></pre></div></div> <p>If we compare them to the coefficients we used to simulate this dataset, we can see that they are indeed very close to one another!</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre> <span class="n">model_results</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">weights</span><span class="p">,</span> <span class="n">reg</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">]).</span><span class="nf">transpose</span><span class="p">(),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">batch gradient descent</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">linear regression</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">true coeffs</span><span class="sh">'</span><span class="p">])</span>
 <span class="n">model_results</span>
</pre></td></tr></tbody></table></code></pre></figure> <table> <thead> <tr> <th>index</th> <th>batch gradient descent</th> <th>linear regression</th> <th>true coeffs</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1.0002417538638582</td> <td>1.0003210378934184</td> <td>1.0</td> </tr> <tr> <td>1</td> <td>1.000179104020437</td> <td>1.000202425f449444</td> <td>1.0</td> </tr> <tr> <td>2</td> <td>2.0206704597237226</td> <td>2.020693020186785</td> <td>2.0</td> </tr> </tbody> </table> <h1 id="example---sgd">Example - SGD</h1> <p>Now we’ll implement SGD. We will adapt the code from earlier, except now each batch is of size 1 (\(m=1\)).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
</pre></td><td class="code"><pre> <span class="c1"># define batch size
</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">1</span>

 <span class="c1"># reinitialize weights
</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

 <span class="c1"># collect avg cost at each epoch
</span> <span class="n">avg_cost_list</span> <span class="o">=</span> <span class="p">[]</span>

 <span class="c1"># collect the updated weights at each epoch
</span> <span class="n">weights_updates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
 <span class="n">weights_updates</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
 <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">epoch = %d</span><span class="sh">'</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
    
     <span class="c1"># collect losses for every batch in epoch
</span>     <span class="n">epoch_loss</span> <span class="o">=</span> <span class="p">[]</span>

     <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">m</span><span class="p">):</span>
         <span class="c1"># predict on training set 
</span>         <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">pred</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">batch</span><span class="p">,:],</span> <span class="n">weights</span><span class="p">)</span>

         <span class="c1"># calculate cost
</span>         <span class="n">cost</span> <span class="o">=</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">batch</span><span class="p">],</span> <span class="n">m</span><span class="p">)</span>
         <span class="n">epoch_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
        
         <span class="c1"># update the weights
</span>         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
             <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="nf">grad</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">batch</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span><span class="n">i</span><span class="p">])</span>
       
     <span class="c1"># save avg cost for epoch across all batches
</span>     <span class="n">avg_cost_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">average</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">))</span>
     <span class="n">weights_updates</span><span class="p">[</span><span class="nb">iter</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

 <span class="n">sgd_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Now let’s plot the avefrage MSE across each training epoch:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">avg_cost_list</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Avg. cost</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Avg. MSE over training epochs - SGD</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should see something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/avg_mse_sgd.png" sizes="95vw"/> <img src="/assets/img/gradient_descent/avg_mse_sgd.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We’ll also plot how the weights changed after each training epoch:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w1</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w2</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w3</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">weights</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Weights over training epochs - SGD</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should see something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/weights_sgd.png" sizes="95vw"/> <img src="/assets/img/gradient_descent/weights_sgd.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the previous example of batch gradient descent, it looked like our model began to converge around 200 training epochs. Here, because we updated the model weights after evaluating each sample, we appear to have actually minimized the cost very early on in the training process. However, the weights that the model has converged on seem to be slightly less accurate compared to batch gradient descent. Let’s add these results to the data frame and take a look:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre> <span class="n">model_results</span><span class="p">[</span><span class="sh">'</span><span class="s">SGD</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>
 <span class="n">model_results</span>
</pre></td></tr></tbody></table></code></pre></figure> <table> <thead> <tr> <th>index</th> <th>batch gradient descent</th> <th>linear regression</th> <th>true coeffs</th> <th>SGD</th> <th>mini-batch</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1.0002417538638582</td> <td>1.0003210378934184</td> <td>1.0</td> <td>1.1791146471366072</td> <td>0.9970121830081912</td> </tr> <tr> <td>1</td> <td>1.000179104020437</td> <td>1.000202425449444</td> <td>1.0</td> <td>1.0082643415913106</td> <td>1.0008372545371162</td> </tr> <tr> <td>2</td> <td>2.0206704597237226</td> <td>2.020693020186785</td> <td>2.0</td> <td>1.9850734474472451</td> <td>2.0208606244363096</td> </tr> </tbody> </table> <h1 id="example---mini-batch-stochastic-gradient-descent">Example - mini-batch stochastic gradient descent</h1> <p>Now let’s modify the code from the SGD example to accomodate mini-batch gradient descent. Let’s define the batch size as \(m=100\) to yield 100 batches per epoch. First we’ll define a function for partioning our data into batches:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
     <span class="sh">'''</span><span class="s">
     code for partitioning dataset into batcehs
     </span><span class="sh">'''</span>
    
     <span class="c1"># shuffle indices of samples 
</span>     <span class="n">sample_ix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
     <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">sample_ix</span><span class="p">)</span>
    
     <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>

     <span class="k">for</span> <span class="n">batch_ix</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="o">//</span><span class="n">m</span><span class="p">):</span>
         <span class="c1"># determine which samples to pick for batch
</span>         <span class="n">samples_in_batch</span> <span class="o">=</span> <span class="n">sample_ix</span><span class="p">[</span><span class="n">batch_ix</span><span class="o">*</span><span class="n">m</span><span class="p">:(</span><span class="n">batch_ix</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="n">m</span> <span class="p">)]</span>
         <span class="n">batches</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">samples_in_batch</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="n">samples_in_batch</span><span class="p">]])</span>

     <span class="k">return</span> <span class="n">batches</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Now we’ll modify the code from our SGD example to implement mini-batch gradient descent:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
</pre></td><td class="code"><pre> <span class="c1"># define batch size
</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>

 <span class="c1"># reinitialize weights
</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

 <span class="c1"># collect avg cost at each epoch
</span> <span class="n">avg_cost_list</span> <span class="o">=</span> <span class="p">[]</span>

 <span class="c1"># collect the updated weights at each epoch
</span> <span class="n">weights_updates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
 <span class="n">weights_updates</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="c1"># get batched data
</span> <span class="n">batches</span> <span class="o">=</span> <span class="nf">batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

 <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

 <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">epoch = %d</span><span class="sh">'</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
    
     <span class="c1"># collect losses for every batch in epoch
</span>     <span class="n">epoch_loss</span> <span class="o">=</span> <span class="p">[]</span>

     <span class="k">for</span> <span class="n">batch_ix</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">m</span><span class="p">):</span>
    
         <span class="n">batch_X</span> <span class="o">=</span> <span class="n">batches</span><span class="p">[</span><span class="n">batch_ix</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
         <span class="n">batch_y</span> <span class="o">=</span> <span class="n">batches</span><span class="p">[</span><span class="n">batch_ix</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

         <span class="c1"># predict on training set 
</span>         <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">pred</span><span class="p">(</span><span class="n">batch_X</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

         <span class="c1"># calculate cost
</span>         <span class="n">cost</span> <span class="o">=</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
         <span class="n">epoch_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
        
         <span class="c1"># update the weights
</span>         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
             <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="nf">grad</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">,</span> <span class="n">batch_X</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>
    
     <span class="c1"># save avg cost for epoch across all batches
</span>     <span class="n">avg_cost_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">average</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">))</span>
     <span class="n">weights_updates</span><span class="p">[</span><span class="nb">iter</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">weights</span>

 <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

 <span class="n">minibatch_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Now we’ll plot the average MSE at each epoch:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">avg_cost_list</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Avg. cost</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Avg. MSE over training epochs - mini-batch gradient descent</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/mini_batch_mse.png" sizes="95vw"/> <img src="/assets/img/gradient_descent/mini_batch_mse.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And how the weights changed after each epoch:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w1</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w2</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">weights_updates</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">w3</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">weights</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Weights over training epochs - mini-batch gradient descent</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient_descent/mini_batch_weights.png" sizes="95vw"/> <img src="/assets/img/gradient_descent/mini_batch_weights.png" class="img-fluid" width="400" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Finally, we’ll compare the results we got from batch gradient descent to the other approaches:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre> <span class="n">model_results</span><span class="p">[</span><span class="sh">'</span><span class="s">mini-batch</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>
 <span class="n">model_results</span>
</pre></td></tr></tbody></table></code></pre></figure> <table> <thead> <tr> <th>index</th> <th>batch gradient descent</th> <th>linear regression</th> <th>true coeffs</th> <th>SGD</th> <th>mini-batch</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1.0002417538638582</td> <td>1.0003210378934184</td> <td>1.0</td> <td>1.1791146471366072</td> <td>0.9970121830081912</td> </tr> <tr> <td>1</td> <td>1.000179104020437</td> <td>1.000202425449444</td> <td>1.0</td> <td>1.0082643415913106</td> <td>1.0008372545371162</td> </tr> <tr> <td>2</td> <td>2.0206704597237226</td> <td>2.020693020186785</td> <td>2.0</td> <td>1.9850734474472451</td> <td>2.0208606244363096</td> </tr> </tbody> </table> <p>Let’s also compare the time it took to run these three different types of gradient descent:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre> <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">batch gradient descent time: %.3f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">batch_gd_time</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">SGD time: %.3f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">sgd_time</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">mini batch gradient descent time: %.3f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">minibatch_time</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>This will give you something that looks like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>batch gradient descent time: 35.231
SGD time: 517.481
mini batch gradient descent time: 6.185
</code></pre></div></div> <p>Overall, all methods managed to converge on weights that are close to the true values we used to generate the data, and also comparable to the results of using sklearn’s linear regression function. However, we observed that SGD and mini-batch appeared to converge faster. Using SGD and mini-batch gradient descent, we could have probably reduced the number of epochs to reach the optimal weights in less time. SGD took very long to run because we updated the model weights after evaluating every single sample, and every single sample was evaluated 1000 times. Mini-batch gradient descent was much faster, and demonstrates that it is a good mix of the pros and cons of batch gradient descent and SGD.</p>]]></content><author><name></name></author><category term="tutorials"/><category term="code"/><category term="machine_learning"/><category term="gradient_descent"/><category term="backpropagation"/><category term="jupyter"/><category term="code"/><summary type="html"><![CDATA[A Python-coded implementation of gradient descent for a linear regression on simulated data]]></summary></entry><entry><title type="html">A very, very basic neural network in Python</title><link href="https://zrcjessica.github.io/blog/2023/python_nn/" rel="alternate" type="text/html" title="A very, very basic neural network in Python"/><published>2023-02-20T21:01:00+00:00</published><updated>2023-02-20T21:01:00+00:00</updated><id>https://zrcjessica.github.io/blog/2023/python_nn</id><content type="html" xml:base="https://zrcjessica.github.io/blog/2023/python_nn/"><![CDATA[<p>When I first started learning about generalized linear models and optimization, my PI advised me to try implementing my own examples from scratch. I found this approach very helpful and have since started using this approach to try to gain a better understand of how a number of different aspects of machine learning work. We have so many powerful libraries for machine learning these days that sometimes you can get by with just a surface level understanding of how things work. There’s nothing wrong with that, but I sometimes get hung up when I don’t understand the details of how something works. Here, I’ve adapted an exercise from an excellent <a href="http://iamtrask.github.io/2015/07/12/basic-python-network/">post by Andrew Trask</a> on a toy example of a neural network implemented from scratch with Python.</p> <h1 id="task">Task</h1> <p>In this example, we are training a neural network to make a binary prediction (\(\{0,1\}\)) based on a binary input vector of size \(3\). The training data provided is:</p> <table class="tg"> <tr> <th class="tg-5rcs" colspan="3">Inputs</th> <th class="tg-5rcs">Output</th> </tr> <tr> <td class="tg-4kyz">0</td> <td class="tg-4kyz">0</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">0</td> </tr> <tr> <td class="tg-4kyz">1</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">1</td> </tr> <tr> <td class="tg-4kyz">1</td> <td class="tg-4kyz">0</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">1</td> </tr> <tr> <td class="tg-4kyz">0</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">1</td> <td class="tg-4kyz">0</td> </tr> </table> <p>From looking at this table, one might notice a pattern - the output is 1 whenever the first value in the input vector is also 1, suggesting a correlation between these two values. By training a neural network, it should also be able to learn this pattern.</p> <h1 id="the-neural-network-architecture">The neural network architecture</h1> <p>Our simple neural network will sipmly pass the inputs through a single neuron with a sigmoid activation function to return an output between 0 and 11. The neural network architecture can be visualized as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/python_nn/nn_arch.png" sizes="95vw"/> <img src="/assets/img/python_nn/nn_arch.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The first layer is the input channel, where each of the values in the input vector are fed into the neural network, and the second layer produces the output based on a transformation of the inputs.</p> <h1 id="training-the-neural-network">Training the neural network</h1> <p>To train the neural network, we will use every data point (row) in the data table and pass it through the network forwards and backwards \(n\) times. Over each pass, or iteration, we will update the weights in the neural network with the goal of improving the outputs at each iteration.</p> <h2 id="forward-propagation">Forward propagation</h2> <p>During forward propagation, each input row of our table (corresponds to one sample data point) will be passed through the first layer and multiplied by the hidden weights in the network via a dot product. There is a different weight for each input, producing a weighted sum:</p> \[s = \Sigma w_i x_i = w_1 x_1 + w_2 x_2 + w_3 x_3\] <p>This weighted sum is then passed to a sigmoid activation function to output a prediction, \(\hat{y} = \sigma(s)\). We will repeat this for \(n\) trianing iterations.</p> <h3 id="initializing-the-weights">Initializing the weights</h3> <p>The weights are the parameters of our neural network which we will be tuning over the course of training to try to get the most accurate predictions possible. We will start by initializing these with very small values (for simplicity’s sake, we will generate random values with a mean of 0).</p> <h3 id="activation-function">Activation function</h3> <p>As aforementioned we will be using a sigmoid activation function, which is defined as:</p> \[\sigma(x) = \frac{1}{1+e^{-x}}\] <p>Sigmoid functions are popular in machine learning because they yield an output between 0 and 1 and are also easily differentiable as:</p> \[\sigma ^\prime (x) = x(1-x)\] <p>The derivative is important for the backpropgation step of this neural network, which is the step that actually helps us update the weights in each iteration of the training process.</p> <h2 id="back-propagation">Back propagation</h2> <p>This is the part where we work backwards from the predicted output of the neural network to update the weights.</p> <h3 id="calculating-the-error">Calculating the error</h3> <p>After each forward pass through the network, we will get a prediction for each sample in the training data from the sigmoid function. From this, we can measure the error of the prediction given the current weights as the different between the prediction and the true output value and then use this information to update the weights. In machine learning, there are various different <strong>cost functions</strong> for measuring the difference between true and predicted values. While the original blog post simply refers to the error as the difference between the true and predicted outputs, I will use a squared error <strong>cost function</strong> and demonstrate further along this post why these approaches are equivalent. The squared error cost function I will use is expressed as:</p> \[E = \frac{1}{2}(y-\hat{y})^2\] <p>Here, \(y\) represents the true output while \(\hat{y}\) is the predicted output for a given training iteraction. In a more sophisticated neural network with multiple output channels, the error would be summed across all outputs, but our neural network only has one output so the expression is simpler. We add \(\frac{1}{2}\) to the beginning of this function because this function will be differentiated and the fraction allows the exponent to be canceled when differentiated. Because the derivative is typically multiplied by a learning rate anyways, it doesn’t matter that a constant is introduced here.</p> <h3 id="updating-the-weights---gradient-descent">Updating the weights - gradient descent</h3> <p>The magnitude of the error from the cost function will let us know how we should adjust the current weights in the network to achieve more accurate predictions. That is, we want to adjust the weights, \(\mathbf{w}\), in response to the error, \(E\). Essentially, we’re asking the question: <strong>how does the error change as the weights change?</strong> The mathematical of this expression is \(\frac{ \partial{E} }{ \partial{w_i} }\), or the partial derivative of \(E\) with respect to a given weight \(w_i\). We can calculate this using a method called <strong>gradient descent</strong>. Gradient descent follows the curve of a function to find the set of inputs that yields the minimum output of the function. In our case, we are following the curve of the cost function to find the weights that yield the minimum error! Gradient is another way of referring to the derivative of a function - it follows that gradient descent is executed using derivatives.</p> <h4 id="example">Example</h4> <p>To get a clearer idea of how gradient descent works, we will walk through the process of updating a single weight, \(w_1\), in our model. In order to do this, we need to find all of the intermediate functions that relate \(E\) to \(w_1\). It might be easier if we visualize this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/python_nn/backprop_diagram.png" sizes="95vw"/> <img src="/assets/img/python_nn/backprop_diagram.png" class="img-fluid" width="500" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We know that the cost function is calculated from \(y\) and \(\hat{y}\), that \(hat{y}\) is calculated as \(\sigma(s)\), and that \(s\) is simply the weighted sum of the inputs and the weights \(w_1 x_1 + w_2 x_2 + w_3 x_3\). Now we have all of the functions that relate \(E\) and \(w_1\)! The reason why we want all of the intermediate functions is because we will be using the chain rule (from calculus) to calculate \(\frac{\partial{E}}{\partial{w_1}}\):</p> \[\frac{\partial{E}}{\partial{w_1}} = \frac{\partial{E}}{\partial{\hat{y}}} \frac{\partial{\hat{y}}}{\partial{s}} \frac{\partial{s}}{\partial{w_1}}\] <p>The chain rule essentially breaks things down into a product of the partial derivatives of all of the intermediate functions relating \(E\) to \(w_1\).</p> <p>The partial derivative of \(E\) with respect to \(\hat{y}\) is:</p> \[\frac{\partial{E}}{\partial{\hat{y}}} = 2*\frac{1}{2}(y-\hat{y})^{2-1}*-1 = \hat{y}-y\] <p>The partial derivative of \(\hat{y}\) with respect to \(s\) is:</p> \[\frac{\partial{\hat{y}}}{\partial{s}} = \sigma ^\prime (\hat{y}) = \hat{y}(1-\hat{y})\] <p>Finally, the partial derivative of \(s\) with respect to \(w_1\) is:</p> \[\frac{\partial{s}}{\partial{w_1}}=x_1+0+0 = x_1\] <p>Therefore, for a given training iteration \(j\), we will generate an updated value of \(w_{1_{j+1}}\) by subtracting \(\frac{\partial{E}}{\partial{w_1}}\) from the value of \(w_1\) in the current iteration (\(w_{1_j}\)). This is because moving in the <strong>opposite</strong> direction of the gradient of the error will help us reach a minimum error. By extension, the value of \(\frac{\partial{E}}{\partial{w_i}}\) is the quantity by which we update any given weight \(w_i\). Typically in gradient descent, the gradient will also be multiplied by a carefully chosen step size, or learning rate, \(\eta\), but for the purpose of our example we will use \(\eta=1\). This will end up working out fine for our example, but it may not be the best learning rate for more complicated tasks and datasets.</p> <h4 id="revisiting-the-original-example">Revisiting the original example</h4> <p>In Andrew Trask’s original post, he updates the weights according to the formula: \(input * error * output(1-output)\). We can see that this expression is equivalent to the expression we have derived using gradient descent. The \(input\) in his expresison is equivalent to \(x_i\) in ours; he defined \(error\) as the difference between the true and predicted outputs, or \(\hat{y}-y\) in ours; and \(output\) in his expression is simply \(\hat{y}\) in ours. I was originally confused by where his formula for updating the weights came from, so I wanted to sit down and really prove where it came from to make sure I understood. Hope this helps someone else out there too!</p> <h3 id="summing-over-a-batch">Summing over a batch</h3> <p>During each iteration of our training loop, we will evaluate all four samples (this is known as <strong>batch gradient descent</strong>). How do we integrate data from the four samples in one batch to make a single update to each value of \(w\)? For a given weight \(w_i\), an input value at the corresponding position \(x_i\) will yield a different value of \(\frac{\partial{E}}{\partial{w_i}}\) for updating the weights. We will simply sum the values obtained across all samples for each \(w_{i_j}\) to obtain a single update quantity \(w_{i_{j+1}}\). We will implement this in the code with vectorization.</p> <h1 id="implementation">Implementation</h1> <p>Here is the code! We will work on implementing this simple neural network with the provided training data using only numpy (and matplotlib for visualization). This tutorial can also be found as a <a href="https://github.com/zrcjessica/ml_concepts/blob/main/python_nn.ipynb">Jupyter notebook on GitHub</a>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
</pre></td><td class="code"><pre> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

 <span class="c1"># random seed for reproducibility
</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

 <span class="c1"># a class for the simple two-layer neural network
</span> <span class="k">class</span> <span class="nc">SimpleNN</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    initialize a simple feed-forward two layer neural network:
    - input layer of variable size
    - output layer of fixed size (1)
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        initialize weights in network and declare input size
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="c1"># random weights with mean = 0
</span>        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">((</span><span class="n">input_size</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">-</span><span class="mi">1</span>
    
    <span class="k">def</span> <span class="nf">__sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">weighted_sum</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        sigmoid function intended to take as input weight sum of weights and inputs
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">weighted_sum</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">__gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        computes derivative of sigmoid function for a given value of x
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__accuracy</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        given true and predicted outputs, calculate accuracy
        </span><span class="sh">"""</span>
        <span class="c1"># binarize predicted values
</span>        <span class="n">pred_bin</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">pred</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_bin</span> <span class="o">==</span> <span class="n">target</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span><span class="o">/</span><span class="n">target</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">accuracy</span>
        
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">,</span> <span class="n">iters</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        function for training neural network on training data
        </span><span class="sh">"""</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
            <span class="c1">### feed forward ###
</span>            
            <span class="c1"># calculate weighted sum
</span>            <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
            
            <span class="c1"># predict yhat
</span>            <span class="n">yhat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__sigmoid</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            
            <span class="c1">### backprop ###
</span>            
            <span class="c1"># calculate cost function
</span>            <span class="n">err</span> <span class="o">=</span> <span class="n">trainY</span> <span class="o">-</span> <span class="n">yhat</span>
            
            <span class="c1"># gradient descent
</span>            <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">trainX</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">trainY</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="nf">__gradient</span><span class="p">(</span><span class="n">yhat</span><span class="p">))</span>
            
            <span class="c1"># update weights
</span>            <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="n">grad</span> 
            
            <span class="c1"># calculate accuracy
</span>            <span class="n">accu_iter</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__accuracy</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">trainY</span><span class="p">)</span>
            <span class="n">accuracy</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accu_iter</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">iter</span><span class="o">%</span><span class="mi">100</span> <span class="o">==</span><span class="mi">0</span> <span class="ow">or</span> <span class="nb">iter</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">iter %d</span><span class="sh">"</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">)</span>
                    <span class="nf">print</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">accuracy = %.2f</span><span class="se">\n</span><span class="sh">"</span> <span class="o">%</span> <span class="n">accu_iter</span><span class="p">)</span>
            
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">output after training:</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy</span>
            
    <span class="k">def</span> <span class="nf">pred</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">testX</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        given a previously unseen set of input data, predict output
        </span><span class="sh">"""</span>
        <span class="c1"># calculate weighted sum of inputs and weights
</span>        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
        
        <span class="c1"># predict yhat
</span>        <span class="n">yhat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">__sigmoid</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">yhat</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>We’ll start by initializing our neural network and checking out the starting weights:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="n">nn</span> <span class="o">=</span> <span class="nc">SimpleNN</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
 
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">starting weights</span><span class="sh">"</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The output will look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>starting weights
[[-0.16595599]
 [ 0.44064899]
 [-0.99977125]]
</code></pre></div></div> <p>Now let’s load in the training data as numpy arrays:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre> <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
              
 <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]).</span><span class="n">T</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Now we’ll train the network over 1,000 training iterations!</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre> <span class="n">nn</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You’ll get an output that looks something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output after training:
[[0.03178421]
 [0.97414645]
 [0.97906682]
 [0.02576499]]
</code></pre></div></div> <p>What are the weights after training and how have they changed from the initial weights? We can check with the function <code class="language-plaintext highlighter-rouge">nn.weights</code>, which will give you something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[ 7.26283009],
       [-0.21614618],
       [-3.41703015]])
</code></pre></div></div> <p>We can see that \(w_1\) is by far larger than \(w_2\) and \(w_3\), which is in line with the relationship we observed between \(x_1\) and the output \(y\) at the beginning of this tutorial.</p> <p>We can also try visualizing the accuracy over each training iteration:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">accuracy</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">training iter</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>You should get something that looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/python_nn/acc_plot.png" sizes="95vw"/> <img src="/assets/img/python_nn/acc_plot.png" class="img-fluid" width="500" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The accuracy improves pretty quickly here, likely because we have such a simple example.</p> <h1 id="prediction">Prediction</h1> <p>Just to make it interesting, let’s try predicting on a new data point where the input vector is \({1,0,0}\) and the corresponding true output is \(y=1\).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="n">testX</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
 <span class="n">testY</span> <span class="o">=</span> <span class="mi">1</span>

 <span class="n">nn</span><span class="p">.</span><span class="nf">pred</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>If the predicted output is very close to 1, the binarizing this probability will yield a match very close to the true target value of this data point. The neural network has been trained the identify the pattern in the data and make accurate predictions!</p>]]></content><author><name></name></author><category term="tutorials"/><category term="code"/><category term="machine_learning"/><category term="deep_learning"/><category term="neural_network"/><category term="gradient_descent"/><category term="backpropagation"/><category term="jupyter"/><category term="code"/><summary type="html"><![CDATA[A Python-coded implementation of a two layer neural network using only numpy.]]></summary></entry></feed>